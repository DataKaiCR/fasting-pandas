{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasting Pandas - A guide into optimizing your analytical processing \n",
    "\n",
    "### Part 2\n",
    "\n",
    "---\n",
    "\n",
    "Now that we know how to handle the panda, it's time to share it with the world by saving it into a file. Maybe we want to use that file later on to do some other work. Just another day at the office, or your home. Everyone works remote now, right?\n",
    "\n",
    "Nooo, not yet. We need to take a step back and look into parsing first. \n",
    "\n",
    "### A new problem\n",
    "\n",
    "Our skills have grown and now we are blazing fast into our analysis. But the reality is we still don't know what we are doing and got greedy by trying to work with a bigger dataset. We thought we could handle it but our computer's RAM is starting to melt.\n",
    "\n",
    "Let's create a dataset of 10 million rows, way beyond the Excel realm and analyze it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasting_pandas as fp\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Dtype         \n",
      "---  ------  -----         \n",
      " 0   size    object        \n",
      " 1   age     int32         \n",
      " 2   team    object        \n",
      " 3   result  object        \n",
      " 4   date    datetime64[ns]\n",
      " 5   prob    float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int32(1), object(3)\n",
      "memory usage: 419.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# %timeit /\n",
    "df = fp.generate_results(10_000_000)\n",
    "df.info()\n",
    "# df.to_csv(os.path.join(fp.DATA_DIR, 'slow.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so our 10 million row 6 column dataset is consuming 420mb of RAM just to have it available as a dataframe. You might think this is not a lot, but when merging or applying any of our cool transformations memory spikes start to happen and trouble knocks down the front door. What if we had to work with 20, 50 or 100 million? Yeah, not good.\n",
    "\n",
    "So what do we do?\n",
    "\n",
    "Let's start with the obvious. Our first question should be what should we expect from our outcome. Do we really need all these fields for our analysis? If we don't, then we should consider filtering unnecessary columns. This will help our memory efficiency issues. \n",
    "\n",
    "As a side note, this would be analogous to using the select * statement in SQL. Are you doing that? No! Bad analyst; bad analyst!\n",
    "\n",
    "For our example we will save the dataset as a csv and read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   size    object\n",
      " 1   age     int64 \n",
      " 2   date    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 228.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(os.path.join(fp.DATA_DIR,'slow.csv'))\n",
    "df = pd.read_csv(os.path.join(fp.DATA_DIR,'slow.csv'), usecols = ['size', 'age', 'date'])\n",
    "df.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, something quite interesting has happened. Can you spot it?\n",
    "\n",
    "Yes yes, our memory usage was cut down by almost half which makes sense since we removed half the columns. But something else changed, and if your answer was the datatype of age changed from int32 to int64 then good for you Sherlock, you make me proud.\n",
    "\n",
    "When we first generated our dataframe we did it by calling a function. This function provided the DataFrame the logic it needed to create itself, and the dataframe object assigned a data type to each field. When generating age pandas decided it would parse them as an int32 type.\n",
    "\n",
    "When we saved the csv, the csv encoder read the memory types but also used it's own set of rules to save the file. It incremented the byte size from 32 bits to 64 bits, and when we read it back, the Dataframe was created by parsing the instructions provided by the csv file. So even though we did good by using only the columns we needed, some of our benefits were stained by using a unnecessary data type for our age field.\n",
    "\n",
    "### Downcasting numbers\n",
    "The takeaway is that we should be aware of the data we are dealing with, and based on it's extremes we can assign appropiately the size of the data type field in order to save memory.\n",
    "\n",
    "#### Integers\n",
    "- Int8 stores integers from -128 to 127\n",
    "- Int16 (short) stores integers from -32,768 to 32,767\n",
    "- Int32 stores integers from -2,147,483,648 to 2,147,483,647 \n",
    "- Int64 (long) stores integers from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807\n",
    "#### Floats\n",
    "- Float16 (short) stores 3 decimal places\n",
    "- Float32 (single) stores 7 decimal places\n",
    "- Float64 (double) stores 15 decimal places\n",
    "- Float128 (long double) stores 19 decimal places\n",
    "\n",
    "So, let's do that real quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   size    object\n",
      " 1   age     int32 \n",
      " 2   date    object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 190.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(os.path.join(fp.DATA_DIR,'slow.csv'), usecols = ['size', 'age', 'date'],dtype={'age': 'int32'})\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. So we could have saved 30mb just by being aware of the change on the datatype. But lets use our brains for a second.\n",
    "\n",
    "It seems age is an interesting contender to parse as an int8, since no one should have an age of negative nature or greater than 127 years. Maybe some time in the future when we turn into robots. For now, it seems reasonable to work within these constraints. Lets do a quick comparison to see how much difference there is between different int data byte sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Series name: age\n",
      "Non-Null Count     Dtype\n",
      "--------------     -----\n",
      "10000000 non-null  int8 \n",
      "dtypes: int8(1)\n",
      "memory usage: 9.5 MB\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Series name: age\n",
      "Non-Null Count     Dtype\n",
      "--------------     -----\n",
      "10000000 non-null  int16\n",
      "dtypes: int16(1)\n",
      "memory usage: 19.1 MB\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Series name: age\n",
      "Non-Null Count     Dtype\n",
      "--------------     -----\n",
      "10000000 non-null  int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 38.1 MB\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Series name: age\n",
      "Non-Null Count     Dtype\n",
      "--------------     -----\n",
      "10000000 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 76.3 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].astype('int8').info(),df['age'].astype('int16').info(),df['age'].astype('int32').info(),df['age'].astype('int64').info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 9.5 to 76.3. That is almost a 9x difference of unnecesary memory waste. Feels bad, doesn't it.\n",
    "\n",
    "Let's look at our other colums, size and date. Currently they are parsed as objects, which is default behaviour. So the question is, can we do better?\n",
    "\n",
    "Yes, yes we can.\n",
    "\n",
    "Enter the categorical type. In essence, a category is a fixed number of possible and limited values. I don't want to go to deep into it, but the main benefit is you will experience a drastic improvement on memory usage when reading the file.\n",
    "\n",
    "But..\n",
    "\n",
    "Categorical columns are a very fragile thing. It's very easy parsing back to object columns when you apply transformations, so you could perfectly experience the same memory spike when working with the column indifferent of it's type. This power comes with responsability, and your first one is to go and look into it by yourself.\n",
    "\n",
    "Having said that, I will show you now the difference when parsing a column as an object or category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Series name: date\n",
      "Non-Null Count     Dtype   \n",
      "--------------     -----   \n",
      "10000000 non-null  category\n",
      "dtypes: category(1)\n",
      "memory usage: 19.2 MB\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Series name: date\n",
      "Non-Null Count     Dtype \n",
      "--------------     ----- \n",
      "10000000 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 76.3+ MB\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Series name: date\n",
      "Non-Null Count     Dtype         \n",
      "--------------     -----         \n",
      "10000000 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1)\n",
      "memory usage: 76.3 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.date.astype('category').info(),df.date.astype('object').info(), df.date.astype('datetime64[ns]').info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant, yes. \n",
    "\n",
    "Putting it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Dtype   \n",
      "---  ------  -----   \n",
      " 0   size    category\n",
      " 1   age     int16   \n",
      " 2   date    category\n",
      "dtypes: category(2), int16(1)\n",
      "memory usage: 47.8 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(fp.DATA_DIR,'slow.csv'), usecols=['size', 'age', 'date'], dtype={'age': 'int16', 'size': 'category', 'date': 'category'})\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduced our dataframe memory consumption from 420mb to 48mb. So much room for possibilities!\n",
    "\n",
    "All right, so data type parsing is important. But parsing data like a pro while working with csvs is pretty comical to say the least. \n",
    "\n",
    "I will show you the way. Nobody will be able to say we trained you, as a joke.\n",
    "\n",
    "Go to lesson 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d732ae5492d83632ed3824ca97dd3702300df556ed6848d417127f3581d5ee1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
