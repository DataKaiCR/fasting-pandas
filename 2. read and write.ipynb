{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasting Pandas - A guide into optimizing your analytical processing \n",
    "\n",
    "### Part 3\n",
    "\n",
    "---\n",
    "\n",
    "We now dive into the realm of files. We want to show we are better than those pesky excel and csv pencil pushers. This is an important step to start giving an impression of actually knowing something.\n",
    "\n",
    "Before going any further, I will outright tell you I'm not going to talk about databases. This is out of the scope for this tutorial, for now. First buy me some coffee and dinner and I will think about it.\n",
    "\n",
    "All right, so why did we look into parsing before file management? The main reason is that parsing will have a direct impact on the efficiency and benefits of file formats, so let's start by generating a dataframe. We will now increment our dataset to 20 million rows.\n",
    "\n",
    "Then, a quick comparison on memory optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import fasting_pandas as fp\n",
    "from fasting_pandas import datasets as ds\n",
    "reload(ds)\n",
    "reload(fp)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000000 entries, 0 to 19999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Dtype         \n",
      "---  ------  -----         \n",
      " 0   size    object        \n",
      " 1   age     int32         \n",
      " 2   team    object        \n",
      " 3   result  object        \n",
      " 4   date    datetime64[ns]\n",
      " 5   prob    float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int32(1), object(3)\n",
      "memory usage: 839.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = fp.generate_results(20_000_000)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000000 entries, 0 to 19999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Dtype         \n",
      "---  ------  -----         \n",
      " 0   size    category      \n",
      " 1   age     int8          \n",
      " 2   team    category      \n",
      " 3   date    datetime64[ns]\n",
      " 4   prob    float16       \n",
      " 5   win     bool          \n",
      "dtypes: bool(1), category(2), datetime64[ns](1), float16(1), int8(1)\n",
      "memory usage: 267.0 MB\n"
     ]
    }
   ],
   "source": [
    "fp.set_dtypes_for_results(df)\n",
    "df.drop(axis = 1, columns = 'result', inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so parsing reduced our dataset memory from 840mb to 250mb. All good, but now we want to save our work for future transformations or just for sharing. What now?\n",
    "\n",
    "## CSV\n",
    "\n",
    "CSVs are in the forefront of data analytics. Chances are you have encountered one of these bad boys in the wild, and you had to read it. That is if you are lucky. There are stories of having to read multiple csvs from a folder and having to merge them into a single dataframe. Sometimes I think we are paying for mistakes we made on a past life.\n",
    "\n",
    "Anywho, for comparison purposes we will measure how much time it takes to save and read csvs. Also we will look into the size of the files to understand how much physical memory we are wasting.\n",
    "\n",
    "I want to keep things simple so quick disclaimer. Check file command is different depending on the OS so:\n",
    "- Windows\n",
    " %ls -GFlash [filename]\n",
    "\n",
    "- Linux:\n",
    " !ls -GFlash [filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "# Disclosure. This takes a long time to execute since it loops 7 times. Don't waste your life on this and just take my word. Or don't, you do you.\n",
    "df.to_csv(os.path.join(fp.DATA_DIR,'dataset.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Work_1TB\n",
      " Volume Serial Number is 9C92-468C\n",
      "\n",
      " Directory of d:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\n",
      "\n",
      "\n",
      " Directory of d:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\data\n",
      "\n",
      "03/25/2023  11:50 PM       786,079,237 dataset.csv\n",
      "               1 File(s)    786,079,237 bytes\n",
      "               0 Dir(s)  606,673,330,176 bytes free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    }
   ],
   "source": [
    "%ls -GFlash data\\dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.94 s ± 148 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df_csv = pd.read_csv(os.path.join(fp.DATA_DIR,'dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(os.path.join(fp.DATA_DIR,'dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000000 entries, 0 to 19999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   size    object \n",
      " 1   age     int64  \n",
      " 2   team    object \n",
      " 3   date    object \n",
      " 4   prob    float64\n",
      " 5   win     bool   \n",
      "dtypes: bool(1), float64(1), int64(1), object(3)\n",
      "memory usage: 782.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_csv.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well well well, so lots of interesting things going on with the CSVs.\n",
    "\n",
    "First interesting fact is they are trash.\n",
    "\n",
    "It took 50 seconds to save, the size of the file is 750mb, around 10 seconds to read and it caused havoc on our carefully crafted datatypes and now we are stuck with a grotesque csv that is consuming 780mb of RAM.\n",
    "\n",
    "Lesson, avoid CSVs like the plague unless you don't have a choice. Thankfully, we don't need to use them anymore. Ok thank you byeeeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del df_csv\n",
    "except NameError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove('data/dataset.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "except NameError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.08 s ± 33.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df.to_pickle(os.path.join(fp.DATA_DIR,'dataset.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Work_1TB\n",
      " Volume Serial Number is 9C92-468C\n",
      "\n",
      " Directory of d:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\n",
      "\n",
      "\n",
      " Directory of d:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\data\n",
      "\n",
      "03/25/2023  11:52 PM       280,001,673 dataset.pickle\n",
      "               1 File(s)    280,001,673 bytes\n",
      "               0 Dir(s)  607,459,409,920 bytes free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    }
   ],
   "source": [
    "%ls -GFlash data\\dataset.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ms ± 4.53 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df_pickle = pd.read_pickle(os.path.join(fp.DATA_DIR,'dataset.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle = pd.read_pickle(os.path.join(fp.DATA_DIR,'dataset.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000000 entries, 0 to 19999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Dtype         \n",
      "---  ------  -----         \n",
      " 0   size    category      \n",
      " 1   age     int8          \n",
      " 2   team    category      \n",
      " 3   date    datetime64[ns]\n",
      " 4   prob    float16       \n",
      " 5   win     bool          \n",
      "dtypes: bool(1), category(2), datetime64[ns](1), float16(1), int8(1)\n",
      "memory usage: 267.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_pickle.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle files seem so much more promising. Lets point out the benefits:\n",
    "\n",
    "1. File size is significantly lower. We went down from 780mb to 280mb. Compression is good for going downhill, and for saving your files.\n",
    "2. They are much faster to write. 50 to 1 second drop.\n",
    "3. They are take 100 ms to read.\n",
    "4. Last but not least, they respected the datatype parsing!!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowNotImplementedError",
     "evalue": "Unhandled type for Arrow to Parquet schema conversion: halffloat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mtimeit\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdf.to_parquet(os.path.join(fp.DATA_DIR,\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdataset.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m))\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2430\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2428\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m   2429\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[1;32m-> 2430\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   2432\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2433\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2434\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1164\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m):\n\u001b[0;32m   1163\u001b[0m     number \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m index\n\u001b[1;32m-> 1164\u001b[0m     time_number \u001b[39m=\u001b[39m timer\u001b[39m.\u001b[39;49mtimeit(number)\n\u001b[0;32m   1165\u001b[0m     \u001b[39mif\u001b[39;00m time_number \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m:\n\u001b[0;32m   1166\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:158\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    156\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[0;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[0;32m    159\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[1;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:2976\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2889\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2890\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   2891\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2972\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   2973\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2974\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 2976\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   2977\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2978\u001b[0m     path,\n\u001b[0;32m   2979\u001b[0m     engine,\n\u001b[0;32m   2980\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   2981\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   2982\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[0;32m   2983\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   2984\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2985\u001b[0m )\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:430\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[0;32m    428\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[1;32m--> 430\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[0;32m    431\u001b[0m     df,\n\u001b[0;32m    432\u001b[0m     path_or_buf,\n\u001b[0;32m    433\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    434\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m    435\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[0;32m    436\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    437\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    438\u001b[0m )\n\u001b[0;32m    440\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:204\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mwrite_to_dataset(\n\u001b[0;32m    196\u001b[0m             table,\n\u001b[0;32m    197\u001b[0m             path_or_handle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    201\u001b[0m         )\n\u001b[0;32m    202\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m         \u001b[39m# write to single output file\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mwrite_table(\n\u001b[0;32m    205\u001b[0m             table, path_or_handle, compression\u001b[39m=\u001b[39;49mcompression, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    206\u001b[0m         )\n\u001b[0;32m    207\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[39mif\u001b[39;00m handles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:3071\u001b[0m, in \u001b[0;36mwrite_table\u001b[1;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **kwargs)\u001b[0m\n\u001b[0;32m   3069\u001b[0m use_int96 \u001b[39m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[0;32m   3070\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3071\u001b[0m     \u001b[39mwith\u001b[39;00m ParquetWriter(\n\u001b[0;32m   3072\u001b[0m             where, table\u001b[39m.\u001b[39;49mschema,\n\u001b[0;32m   3073\u001b[0m             filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[0;32m   3074\u001b[0m             version\u001b[39m=\u001b[39;49mversion,\n\u001b[0;32m   3075\u001b[0m             flavor\u001b[39m=\u001b[39;49mflavor,\n\u001b[0;32m   3076\u001b[0m             use_dictionary\u001b[39m=\u001b[39;49muse_dictionary,\n\u001b[0;32m   3077\u001b[0m             write_statistics\u001b[39m=\u001b[39;49mwrite_statistics,\n\u001b[0;32m   3078\u001b[0m             coerce_timestamps\u001b[39m=\u001b[39;49mcoerce_timestamps,\n\u001b[0;32m   3079\u001b[0m             data_page_size\u001b[39m=\u001b[39;49mdata_page_size,\n\u001b[0;32m   3080\u001b[0m             allow_truncated_timestamps\u001b[39m=\u001b[39;49mallow_truncated_timestamps,\n\u001b[0;32m   3081\u001b[0m             compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3082\u001b[0m             use_deprecated_int96_timestamps\u001b[39m=\u001b[39;49muse_int96,\n\u001b[0;32m   3083\u001b[0m             compression_level\u001b[39m=\u001b[39;49mcompression_level,\n\u001b[0;32m   3084\u001b[0m             use_byte_stream_split\u001b[39m=\u001b[39;49muse_byte_stream_split,\n\u001b[0;32m   3085\u001b[0m             column_encoding\u001b[39m=\u001b[39;49mcolumn_encoding,\n\u001b[0;32m   3086\u001b[0m             data_page_version\u001b[39m=\u001b[39;49mdata_page_version,\n\u001b[0;32m   3087\u001b[0m             use_compliant_nested_type\u001b[39m=\u001b[39;49muse_compliant_nested_type,\n\u001b[0;32m   3088\u001b[0m             encryption_properties\u001b[39m=\u001b[39;49mencryption_properties,\n\u001b[0;32m   3089\u001b[0m             write_batch_size\u001b[39m=\u001b[39;49mwrite_batch_size,\n\u001b[0;32m   3090\u001b[0m             dictionary_pagesize_limit\u001b[39m=\u001b[39;49mdictionary_pagesize_limit,\n\u001b[0;32m   3091\u001b[0m             store_schema\u001b[39m=\u001b[39;49mstore_schema,\n\u001b[0;32m   3092\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m writer:\n\u001b[0;32m   3093\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(table, row_group_size\u001b[39m=\u001b[39mrow_group_size)\n\u001b[0;32m   3094\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:990\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[1;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **options)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata_collector \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmetadata_collector\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    989\u001b[0m engine_version \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mV2\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 990\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m _parquet\u001b[39m.\u001b[39;49mParquetWriter(\n\u001b[0;32m    991\u001b[0m     sink, schema,\n\u001b[0;32m    992\u001b[0m     version\u001b[39m=\u001b[39;49mversion,\n\u001b[0;32m    993\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    994\u001b[0m     use_dictionary\u001b[39m=\u001b[39;49muse_dictionary,\n\u001b[0;32m    995\u001b[0m     write_statistics\u001b[39m=\u001b[39;49mwrite_statistics,\n\u001b[0;32m    996\u001b[0m     use_deprecated_int96_timestamps\u001b[39m=\u001b[39;49muse_deprecated_int96_timestamps,\n\u001b[0;32m    997\u001b[0m     compression_level\u001b[39m=\u001b[39;49mcompression_level,\n\u001b[0;32m    998\u001b[0m     use_byte_stream_split\u001b[39m=\u001b[39;49muse_byte_stream_split,\n\u001b[0;32m    999\u001b[0m     column_encoding\u001b[39m=\u001b[39;49mcolumn_encoding,\n\u001b[0;32m   1000\u001b[0m     writer_engine_version\u001b[39m=\u001b[39;49mengine_version,\n\u001b[0;32m   1001\u001b[0m     data_page_version\u001b[39m=\u001b[39;49mdata_page_version,\n\u001b[0;32m   1002\u001b[0m     use_compliant_nested_type\u001b[39m=\u001b[39;49muse_compliant_nested_type,\n\u001b[0;32m   1003\u001b[0m     encryption_properties\u001b[39m=\u001b[39;49mencryption_properties,\n\u001b[0;32m   1004\u001b[0m     write_batch_size\u001b[39m=\u001b[39;49mwrite_batch_size,\n\u001b[0;32m   1005\u001b[0m     dictionary_pagesize_limit\u001b[39m=\u001b[39;49mdictionary_pagesize_limit,\n\u001b[0;32m   1006\u001b[0m     store_schema\u001b[39m=\u001b[39;49mstore_schema,\n\u001b[0;32m   1007\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[0;32m   1008\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_open \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\pyarrow\\_parquet.pyx:1753\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Proyectos\\datakai\\projects\\github\\datakaicr\\public\\fasting-pandas\\venv\\Lib\\site-packages\\pyarrow\\error.pxi:121\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowNotImplementedError\u001b[0m: Unhandled type for Arrow to Parquet schema conversion: halffloat"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df.to_parquet(os.path.join(fp.DATA_DIR,'dataset.parquet'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I felt this error on purpose. Parquet files don't work nicely with any float below 32 bits, so we need to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prob'] = df['prob'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.28 s ± 43.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df.to_parquet(os.path.join(fp.DATA_DIR,'dataset.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402 ms ± 9.95 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df_parquet = pd.read_parquet(os.path.join(fp.DATA_DIR,'dataset.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('dataset.parquet')\n",
    "df_parquet = pd.read_parquet('dataset.parquet')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fp.generate_results(20_000_000)\n",
    "fp.set_dtypes_for_results(df)\n",
    "df.drop(axis = 1, columns = 'result', inplace=True)\n",
    "df.to_parquet('dataset_parquet.parquet')\n",
    "df = pd.read_parquet('dataset_parquet.parquet')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -GFlash dataset_parquet.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fp.generate_results(20_000_000)\n",
    "# fp.set_dtypes_for_results(df)\n",
    "# df.drop(axis = 1, columns = 'result', inplace=True)\n",
    "df.to_parquet('dataset_parquet.parquet')\n",
    "df = pd.read_parquet('dataset_parquet.parquet')\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
